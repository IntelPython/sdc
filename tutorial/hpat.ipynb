{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HPAT (High Performance Analytics Toolkit)\n",
    "\n",
    "In this tutorial we'll cover how to use HPAT to accelerate and scale-out data analytics codes.\n",
    "\n",
    "HPAT automatically parallelizes a subset of Python that is commonly used for data analytics and machine learning. It just-in-time-compiles the functions annotated with the @hpat.jit decorator. The decorated functions are replaced with generated parallel MPI binaries that run on bare metal. The supported data structures for large datasets are Numpy arrays and Pandas dataframes.\n",
    "\n",
    "HPAT is based on numba, so we'll start with a quick intro to numba. We'll then dive into HPAT, how it deals with pandas data structures and operations like file-reading, filtering, aggregation etc.\n",
    "\n",
    "## Numba\n",
    "Let's start with a simple example to get familiar with just-in-time-compiling with numba: computing pi using Monte-Carlo Integration.\n",
    "\n",
    "We use numpy arrays and their array-notation (no loop)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calc_pi(n):\n",
    "    x = 2 * np.random.ranf(n) - 1\n",
    "    y = 2 * np.random.ranf(n) - 1\n",
    "    pi = 4 * np.sum(x**2 + y**2 < 1) / n\n",
    "    return pi\n",
    "\n",
    "%timeit calc_pi(2**22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numba is a just-in-time compiler. It can compile functions by inserting hooks through a decorator. When the function is finally called numba will attempt to compile the function to native code for the given input types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba as nb\n",
    "import numpy as np\n",
    "\n",
    "@nb.jit\n",
    "def calc_pi(n):\n",
    "    x = 2 * np.random.ranf(n) - 1\n",
    "    y = 2 * np.random.ranf(n) - 1\n",
    "    pi = 4 * np.sum(x**2 + y**2 < 1) / n\n",
    "    return pi\n",
    "\n",
    "%timeit calc_pi(2**22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not much benefit; most ot the time is spent in numpy which implemented in C anyway.\n",
    "\n",
    "## Parallel Accelerator\n",
    "Numba now has a feature to thread-parallelize data-parallel operations.\n",
    "* __<span style=\"color:red\">In the above cell, try JIT/decorator parameter parallel=True</span>__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel for/range for explicit (maybe higher-level) parallelsim\n",
    "For loops can be marked exiplicitly parallel by replacing range with prange and with parallel=True. Reduction variables are automatically recognized. Race-freedom however must be guaranteed by the programmer.\n",
    "\n",
    "Let's write the same program with an explicit loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba as nb\n",
    "import numpy as np\n",
    "\n",
    "def calc_pi(n):\n",
    "    x = 2 * np.random.ranf(n) - 1\n",
    "    y = 2 * np.random.ranf(n) - 1\n",
    "    s = 0\n",
    "    for i in range(n):\n",
    "        s += x[i]**2 + y[i]**2 < 1\n",
    "    pi = 4 * s / n\n",
    "    return pi\n",
    "\n",
    "%timeit calc_pi(2**22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* While waiting for the result: __<span style=\"color:red\">In the above cell use numba with parallel=True and replace range with nb.prange</span>__. Compared to @nb.jit the parallel option should give you speedup correlating to the number cores in your system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types\n",
    "numba understand some basic data types like integers, floats, lists and most importantly numpy arrays.\n",
    "It cannot compile arbirtrary types, though.\n",
    "\n",
    "We could write the same using pandas data-frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba as nb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "@nb.jit\n",
    "def calc_pi(n):\n",
    "    xy = pd.DataFrame({'x': 2 * np.random.ranf(n) - 1,\n",
    "                       'y': 2 * np.random.ranf(n) - 1})\n",
    "    pi = 4 * xy[xy.x**2 + xy.y**2 < 1].x.count() / n\n",
    "    return pi\n",
    "\n",
    "%timeit calc_pi(2**22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using numba with the above will not lead to any performance benefit: it does not compile to native code.\n",
    "\n",
    "* __<span style=\"color:red\">Try forcing no-python mode by using nopython=True or decorator nb.njit.</span>__\n",
    "\n",
    "## HPAT\n",
    "HPAT advances numba in two dimensions:\n",
    "1. Support for pandas dataframes and operations\n",
    "2. Scaling out to a cluster using MPI\n",
    "\n",
    "Let's start with the same pi example but use HPAT's jit. Notice that in contrast to numba, hpat.jit defaults to parallel=True and nopython=True.\n",
    "\n",
    "### Using Pandas\n",
    "HPAT knows how to handle pandas!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hpat\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "@hpat.jit\n",
    "def calc_pi(n):\n",
    "    xy = pd.DataFrame({'x': 2 * np.random.ranf(n) - 1,\n",
    "                       'y': 2 * np.random.ranf(n) - 1})\n",
    "    pi = 4 * xy[xy.x**2 + xy.y**2 < 1].x.count() / n\n",
    "    return pi\n",
    "\n",
    "%timeit calc_pi(2**22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallelism, Multi-Processing, MPI\n",
    "HPAT know how to parallelize and distribute pandas operations across a cluster. It's using MPI to achieve close-to-native efficienty.\n",
    "\n",
    "Currently running HPAT on multiple processes is not working yet. We need to save our code to a file and start it using mpirun in a (external) shell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hpat\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import process_time as clock\n",
    "\n",
    "@hpat.jit\n",
    "def calc_pi(n):\n",
    "    xy = pd.DataFrame({'x': 2 * np.random.ranf(n) - 1,\n",
    "                       'y': 2 * np.random.ranf(n) - 1})\n",
    "    pi = 4 * xy[xy.x**2 + xy.y**2 < 1].x.count() / n\n",
    "    return pi\n",
    "\n",
    "# warmup call, we do not want to time the compilation time\n",
    "pi = calc_pi(2**22)\n",
    "\n",
    "n = 5\n",
    "\n",
    "t1 = clock()\n",
    "for i in range(5):\n",
    "    pi = calc_pi(2**22)\n",
    "t2 = clock()\n",
    "print('pi={}'.format(pi))\n",
    "\n",
    "print('{:.2f} ms per loop'.format((t2-t1)*1000.0/n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure you provide the right cell number as the last argument!\n",
    "%save -f runme.py ??\n",
    "!mpirun -n 4 python ./runme.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Input\n",
    "<img style=\"float: right;\" src=\"img/file-read.jpg\">\n",
    "Efficient parallel data processing requries data gathering to be parallel as well. HPAT provides parallel and distributed file-reading for different file formats. This tutorial will cover CSV and parquet; HDF5 is supported as well but left to the reader to expore.\n",
    "\n",
    "### Parquet\n",
    "Parquet is a convenient file-format because it not only stores the data but also meta information like data-types.\n",
    "Let's read a file using pandas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "d = pd.read_parquet('cycling_dataset.pq')\n",
    "d.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __<span style=\"color:red\">Now put this into a function, @hpat.jit-compile and call it</span>__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've successfully read a file and returned the number of rows in it.\n",
    "We'll investigate the distribution/parallelism in a bit. Hold back for now.\n",
    "\n",
    "### CSV\n",
    "Let's read the same data from a CSV file. We'll see that type-information is required - we want to compile to native machine code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import hpat\n",
    "\n",
    "@hpat.jit\n",
    "def read_csv():\n",
    "    return len(pd.read_csv('cycling_dataset.csv'))\n",
    "\n",
    "d = read_csv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to provide column names and their types! We will also add a call to hpat.distribution_report() to see how HPAT actually distributes/parallelises the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import hpat\n",
    "import numpy as np\n",
    "\n",
    "@hpat.jit\n",
    "def read_csv():\n",
    "    colnames = ['altitude', 'cadence', 'distance', 'hr', 'latitude', 'longitude', 'power', 'speed', 'time']\n",
    "    coltypes = {'altitude': np.float64,\n",
    "                'cadence': np.float64,\n",
    "                'distance': np.float64,\n",
    "                'hr': np.float64,\n",
    "                'latitude': np.float64,\n",
    "                'longitude': np.float64,\n",
    "                'power': np.float64,\n",
    "                'speed': np.float64,\n",
    "                'time': str}\n",
    "    return pd.read_csv('cycling_dataset.csv', names=colnames, dtype=coltypes, skiprows=1)\n",
    "\n",
    "d = read_csv()\n",
    "hpat.distribution_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution report tells us that all data is actually replicated, e.g. there is no parallelism. The reason for this is that by default any input and output to a HPAT-jit'ed function is replicated on all processes.\n",
    "\n",
    "Try the following (in the above cell)\n",
    "\n",
    "* __<span style=\"color:red\">Removing the return statement will eliminate the entire file read!</span>__\n",
    "* __<span style=\"color:red\">If we return the shape of the dataframe the data will be block partitioned along the first axis. This is because we actually make use of the data but we do not return an entire column/data-frame.</span>__\n",
    "* __<span style=\"color:red\">You can now make the file-name a parameter to the function because we specify column-names and types. Try it!</span>__\n",
    "* __<span style=\"color:red\">Try with MPI!</span>__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%save -f runme.py ??\n",
    "!mpirun -n 4 python ./runme.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: right;\" src=\"img/data-parallel.jpg\">\n",
    "\n",
    "## Simple Data-Parallel Operations\n",
    "Many operations in pandas are trivially data-parallel. There is no communication needed to compute them in parallel.\n",
    "Examples are filtering, combining columns, normlization, dropping columns/rows etc.\n",
    "\n",
    "Let's drop some rows and some columns. Additionally, we create a new column by extracting the month from the time column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import hpat\n",
    "from datetime import datetime\n",
    "\n",
    "@hpat.jit\n",
    "def data_par():\n",
    "    df = pd.read_parquet('cycling_dataset.pq')\n",
    "    df = df[df.power!=0]\n",
    "    df['month'] = df.time.dt.month\n",
    "    df = df.drop(['latitude', 'longitude', 'power', 'time'], axis=1)\n",
    "    return len(df)\n",
    "\n",
    "print(data_par())\n",
    "hpat.distribution_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __<span style=\"color:red\">Try with MPI!</span>__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%save -f runme.py ??\n",
    "!mpirun -n 4 python ./runme.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: right;\" src=\"img/reduction.jpg\">\n",
    "\n",
    "## Reduction operations\n",
    "Reductions, such as avg, sum, mean etc, are mapped to efficient MPI code. Their result gets replicated on all processes.\n",
    "\n",
    "As an example let's compute the mean of the 'power' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import hpat\n",
    "\n",
    "@hpat.jit\n",
    "def mean_power():\n",
    "    df = pd.read_parquet('cycling_dataset.pq')\n",
    "    return df.power.mean()\n",
    "\n",
    "print(mean_power())\n",
    "hpat.distribution_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%save -f runme.py ??\n",
    "!mpirun -n 4 python ./runme.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __<span style=\"color:red\">Try other reductions such as sum()</span>__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: right;\" src=\"img/groupby.jpg\">\n",
    "\n",
    "## GroupBy/Aggregation\n",
    "More challenging for parallel and distributed environments are grouping operations which are typically followed by aggregations/reductions. Like with simple reductions HPAT maps groupby and aggregations to efficient MPI code.\n",
    "\n",
    "Let's compute the average power output per hour. WE return the number of means to avoid REP propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import hpat\n",
    "\n",
    "@hpat.jit\n",
    "def mean_power_pm():\n",
    "    df = pd.read_parquet('cycling_dataset.pq')\n",
    "    df['hour'] = df.time.dt.hour\n",
    "    grp = df.groupby('hour')\n",
    "    mean = grp['power'].mean()\n",
    "    return len(mean)\n",
    "\n",
    "print(mean_power_pm())\n",
    "hpat.distribution_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%save -f runme.py ??\n",
    "!mpirun -n 4 python ./runme.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __<span style=\"color:red\">Try other aggregation/groupby</span>__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: right;\" src=\"img/rolling.jpg\">\n",
    "\n",
    "## Sliding Windows\n",
    "Some popular operations, in particular for time-series analysis, are based on sliding windows, like computing the moving average or percentage change. In a distributed setup these require communication beyond map-reduce. Again, HPAT maps this to efficient patterns known from HPC.\n",
    "\n",
    "Let's compute the moving average of the heart-rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import hpat\n",
    "\n",
    "@hpat.jit\n",
    "def mov_avg():\n",
    "    df = pd.read_parquet('cycling_dataset.pq')\n",
    "    mv_av = df.hr.rolling(4).mean()\n",
    "    return mv_av.mean()\n",
    "\n",
    "print(mov_avg())\n",
    "hpat.distribution_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%save -f runme.py ??\n",
    "!mpirun -n 4 python ./runme.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join\n",
    "HPAT can also efficiently join dataframes.\n",
    "\n",
    "Let's read data, split into 2 dataframes and re-join on time column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import hpat\n",
    "\n",
    "@hpat.jit\n",
    "def merge_dfs():\n",
    "    df = pd.read_parquet('cycling_dataset.pq')\n",
    "    df1 = df[['altitude', 'cadence', 'distance', 'hr', 'time']]\n",
    "    df2 = df[['latitude', 'longitude', 'power', 'speed', 'time']]\n",
    "    df3 = df1.merge(df2, on='time')\n",
    "    return len(df3)\n",
    "\n",
    "print(merge_dfs())\n",
    "hpat.distribution_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%save -f runme.py ??\n",
    "!mpirun -n 4 python ./runme.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using daal4py with HPAT\n",
    "Often data analytics involve some machine learning steps like regression, clustering, classification... \n",
    "We are developing a compatibility package in daal4py which let's use use daal4py in a hpat.jit'ed function.\n",
    "\n",
    "The following code trains a model to predict the power output. daal4py's linear regression supports training on distributed data-sets. We'll use HPAT to distribute the data, preprocess it and then it will hand it over to daal4py which will train a single linear model with the distributed data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import hpat\n",
    "import daal4py as d4p\n",
    "import daal4py.hpat\n",
    "import pandas as pd\n",
    "\n",
    "@hpat.jit\n",
    "def train():\n",
    "    # Read training data\n",
    "    train_set = pd.read_parquet('cycling_train_dataset.pq')\n",
    "    # Remove entries where power==0\n",
    "    train_set = train_set[train_set.power!=0]\n",
    "    # Reduce the dataset, create X.  We drop the target, and other non-essential features.\n",
    "    reduced_dataset = train_set.drop(['time','power','latitude','longitude'], axis=1)\n",
    "    # Get the target, create Y as an 2d array of float64\n",
    "    target = train_set.power.values.reshape(len(train_set),1).astype(np.float64)\n",
    "    \n",
    "    # Create a daal4py linear regression algorithm object\n",
    "    d4p_lm = d4p.linear_regression_training(interceptFlag=True)\n",
    "    # Train the model\n",
    "    lm_trained = d4p_lm.compute(reduced_dataset.values, target)\n",
    "\n",
    "    # Finally return the result\n",
    "    return lm_trained\n",
    "\n",
    "train_result = train()\n",
    "print(train_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%save -f runme.py ??\n",
    "!mpirun -n 4 python ./runme.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full daal4py+HPAT example is provided in a separate notebook (daal4py_data_science.ipynb) and includes the distributed prediction step as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution Annotations\n",
    "Like numba, HPAT accepts typing annotation to the jit decorator. In addition, you can tell HPAT to accept data as already partitioned and distributed and/or return data distributedly, e.g. ungathered. This can be very useful if you want to orchestrate several re-usable functions in a truely SPMD fashion.\n",
    "\n",
    "We saw earlier that returning a data-frame or series will replicate the data-frame on all process and also replacate all related computation. It essentially prohibits parallel/distributed execution. We can tell HPAT to not gather the returned data and instead just return the local portions on each process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import hpat\n",
    "\n",
    "@hpat.jit(distributed=['df'])\n",
    "def read_pq():\n",
    "    df = pd.read_parquet('cycling_dataset.pq')\n",
    "    return df\n",
    "\n",
    "df = read_pq()\n",
    "print(df.shape)\n",
    "hpat.distribution_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%save -f runme.py ??\n",
    "!mpirun -n 4 python ./runme.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intraday-stock-mean-reversion-trading-backtest\n",
    "A nice backtest from http://www.pythonforfinance.net/2017/02/20/intraday-stock-mean-reversion-trading-backtest-in-python/ works nicely with HPAT. The full code use many of the above features and is attached (intraday_mean.py) to the tutorial. Enjoy!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
