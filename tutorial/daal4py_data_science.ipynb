{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilizing HPAT and daal4py in Data Science Workflows\n",
    "\n",
    "The notebook below has been made to demonstrate daal4py in a data science context.  It utilizes a Cycling Dataset for pyworkout-toolkit, and attempts to create a linear regression model from the 5 features collected for telemetry to predict the user's Power output in the absence of a power meter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import sys\n",
    "%matplotlib inline\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example will be exploring workout data pulled from Strava, processed into a CSV for Pandas and daal4py usage.  Below, we utilize pandas to read in the CSV file, and look at the head of dataframe with .head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.read_parquet('cycling_train_dataset.pq')\n",
    "train_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data above has several key features that would be of great use here.  \n",
    "- Altitude can affect performance, so it might be a useful feature.  \n",
    "- Cadence is the revolutions per minute of the crank, and may have possible influence.  \n",
    "- Heart Rate is a measure of the body's workout strain, and would have a high possibly of influence.\n",
    "- Distance may have a loose correlation as it is highly route dependent, but might be possible.\n",
    "- Speed has possible correlations as it ties directly into power."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps with HPAT and daal4py\n",
    "In general, we are trying to predict on the 'power' in Watts to see if we can generate a model that can predict one's power \n",
    "output without the usage of a cycling power meter.\n",
    "1. Load train data and some clean-up\n",
    "2. Linear regression training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and preparing the data\n",
    "\n",
    "In the sections below, we will be using daal4py directly.  After importing the model, we will arrange it in a separate independent and dependent dataframes, then use the daal4py's training class to generate a workable model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.read_parquet('cycling_train_dataset.pq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove entries where power==0\n",
    "train_set = train_set[train_set.power!=0]\n",
    "# Reduce the dataset, create X.  We drop the target, and other non-essential features.\n",
    "reduced_dataset = train_set.drop(['time','power','latitude','longitude'], axis=1)\n",
    "# Get the target, create Y as an 2d array of float64\n",
    "target = train_set.power.values.reshape(len(train_set),1).astype(np.float64)\n",
    "# This is essentially doing np.array(dataset.power.values, ndmin=2).T\n",
    "# as it needs to force a 2 dimensional array as we only have 1 target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X is 5 features by 1991 rows, Y is 1991 rows by 1 column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reduced_dataset.values.shape, target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the Linear Regression Model, and train the model with the data.  We utilize daal4py's linear_regression_training class to create the model, then call .compute() with the independent and dependent data as the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import daal4py as d4p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a linear regression algorithm object\n",
    "d4p_lm = d4p.linear_regression_training(interceptFlag=True)\n",
    "# Train the model\n",
    "lm_trained = d4p_lm.compute(reduced_dataset.values, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model has this number of features: \", lm_trained.model.NumberOfFeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding HPAT\n",
    "Put the above into a function and use the HPAT jit decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First import all the hpat stuff\n",
    "import hpat\n",
    "import daal4py\n",
    "import daal4py.hpat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@hpat.jit\n",
    "def train():\n",
    "    # Read training data\n",
    "    train_set = pd.read_parquet('cycling_train_dataset.pq')\n",
    "    # Remove entries where power==0\n",
    "    train_set = train_set[train_set.power!=0]\n",
    "    # Reduce the dataset, create X.  We drop the target, and other non-essential features.\n",
    "    reduced_dataset = train_set.drop(['time','power','latitude','longitude'], axis=1)\n",
    "    # Get the target, create Y as an 2d array of float64\n",
    "    target = train_set.power.values.reshape(len(train_set),1).astype(np.float64)\n",
    "    \n",
    "    # Create a daal4py linear regression algorithm object\n",
    "    d4p_lm = d4p.linear_regression_training(interceptFlag=True)\n",
    "    # Train the model\n",
    "    lm_trained = d4p_lm.compute(reduced_dataset.values, target)\n",
    "\n",
    "    # Finally return the result\n",
    "    return lm_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_result = train()\n",
    "print(train_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction (inference) with the trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is trained, we can test it with the test part of the dataset.  We drop the same features to match that of the trained model, and put it into daal4py's linear_regression_prediction class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@hpat.jit\n",
    "def predict(model):\n",
    "    # read and clean as before\n",
    "    test_set = pd.read_parquet('cycling_test_dataset.pq')\n",
    "    test_set = test_set[test_set.power!=0]\n",
    "    subset = test_set.drop(['time','power','latitude','longitude'], axis=1)\n",
    "    \n",
    "    # create our prediction algorithm object\n",
    "    lm_predictor = d4p.linear_regression_prediction()\n",
    "    # Now run prediction. The arguments use the independent data and the trained model from above as the parameters.\n",
    "    result = lm_predictor.compute(subset.values, model)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our linear model is the 'model' attribute of the training result\n",
    "pred_result = predict(train_result.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = pd.read_parquet('cycling_test_dataset.pq')\n",
    "test_set = test_set[test_set.power!=0]\n",
    "plt.plot(pred_result.prediction[0:300])\n",
    "plt.plot(test_set.power.values[0:300])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph above shows the Orange (predicted) result over the Blue (original data).  This data is notoriously sparse in features leading to a difficult to predict target!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reminder provides more details on using daal4py.\n",
    "\n",
    "------------------------------------------\n",
    "------------------------------------------\n",
    "\n",
    "## Model properties\n",
    "Another aspect of the model is the trained model's properties, which are explored below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Betas:\",lm_trained.model.Beta) \n",
    "print(\"Number of betas:\", lm_trained.model.NumberOfBetas)\n",
    "print(\"Number of Features:\", lm_trained.model.NumberOfFeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional metrics\n",
    "We can generate metrics on the independent data with daal4py's low_order_moments() class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_processor = d4p.low_order_moments()\n",
    "data = metrics_processor.compute(reduced_dataset.values)\n",
    "data.standardDeviation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Migrating the trained model for inference on external systems\n",
    "\n",
    "Occasionally one may need to migrate the trained model to another system for inference only--this use case allows the training on a much more powerful machine with a larger dataset, and placing the trained model for inference-only on a smaller machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('trained_model2.pickle', 'wb') as model_pi:\n",
    "    pickle.dump(lm_trained.model, model_pi)\n",
    "    model_pi.close"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trained model file above can be moved to an inference-only or embedded system.  This is useful if the training is extreamly heavy or computed-limited.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('trained_model2.pickle', 'rb') as model_import:\n",
    "    lm_import = pickle.load(model_import)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The imported model from file is now usable again.  We can check the betas from the model to ensure that the trained model is present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_import.Beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
