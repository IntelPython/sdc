{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data\n",
    "Install pytorch and torchvision:\n",
    "\n",
    "```bash\n",
    "conda install pytorch torchvision -c pytorch\n",
    "```\n",
    "Download cifar10 data and save to a simple binary file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import os, pickle\n",
    "import numpy as np\n",
    "\n",
    "def create_dataset():\n",
    "    trainset = torchvision.datasets.CIFAR10(root='./data', download=True)\n",
    "    fname = \"./data/cifar-10-batches-py/data_batch_1\"\n",
    "    fo = open(fname, 'rb')\n",
    "    entry = pickle.load(fo, encoding='latin1')\n",
    "    train_data = entry['data']\n",
    "    fo.close()\n",
    "    train_data.tofile(\"train_data.dat\")\n",
    "\n",
    "create_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we load and transform the input data using HPAT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data read time 4.675946950912476\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import hpat\n",
    "from hpat import prange\n",
    "import cv2\n",
    "hpat.multithread_mode = True\n",
    "cv2.setNumThreads(0)  # we use threading across images\n",
    "\n",
    "@hpat.jit(locals={'images:return': 'distributed'})\n",
    "def read_data():\n",
    "    file_name = \"train_data.dat\"\n",
    "    blob = np.fromfile(file_name, np.uint8)\n",
    "    # reshape to images\n",
    "    n_channels = 3\n",
    "    height = 32\n",
    "    width = 32\n",
    "    n_images = len(blob)//(n_channels*height*width)\n",
    "    data = blob.reshape(n_images, height, width, n_channels)\n",
    "\n",
    "    # resize\n",
    "    resize_len = 224\n",
    "    images = np.empty((n_images, resize_len, resize_len, n_channels), np.uint8)\n",
    "    for i in prange(n_images):\n",
    "        images[i] = cv2.resize(data[i], (resize_len, resize_len))\n",
    "\n",
    "    # convert from [0,255] to [0.0,1.0]\n",
    "    # normalize\n",
    "    u2f_ratio = np.float32(255.0)\n",
    "    c0_m = np.float32(0.485)\n",
    "    c1_m = np.float32(0.456)\n",
    "    c2_m = np.float32(0.406)\n",
    "    c0_std = np.float32(0.229)\n",
    "    c1_std = np.float32(0.224)\n",
    "    c2_std = np.float32(0.225)\n",
    "    for i in prange(n_images):\n",
    "        images[i,:,:,0] = (images[i,:,:,0]/ u2f_ratio - c0_m) / c0_std\n",
    "        images[i,:,:,1] = (images[i,:,:,1]/ u2f_ratio - c1_m) / c1_std\n",
    "        images[i,:,:,2] = (images[i,:,:,2]/ u2f_ratio - c2_m) / c2_std\n",
    "\n",
    "    # convert to CHW\n",
    "    images = images.transpose(0, 3, 1, 2)\n",
    "    return images\n",
    "\n",
    "t1 = time.time()\n",
    "imgs = read_data()\n",
    "print(\"data read time\", time.time()-t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `'V:return':'distributed'` annotation indicates that chunks of array `V` are returned in distributed fashion, instead of replicating it which is the default behavior for return. The I/O function `np.fromfile`, as well as all operations on images are parallelized by HPAT.\n",
    " \n",
    "Let's run a simple resnet18 DNN using pretrained weights as an example. We run only on 100 images for faster demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dnn time 10.795934915542603\n"
     ]
    }
   ],
   "source": [
    "from torch import Tensor\n",
    "from torch.autograd import Variable\n",
    "model = torchvision.models.resnet18(True)\n",
    "t1 = time.time()\n",
    "res = model(Variable(Tensor(imgs[:100])))\n",
    "print(\"dnn time\", time.time()-t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use HPAT to get some statistics on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        vals      classes\n",
      "count   100.000000   100.000000\n",
      "mean    6.823207   537.980000\n",
      "std     1.497357   273.730236\n",
      "min     3.747945   36\n",
      "25%     5.845919   365.250000\n",
      "50%     6.838111   553.000000\n",
      "75%     7.639919   774.750000\n",
      "max     13.849208   982\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# get top class stats\n",
    "vals, inds = res.max(1)\n",
    "import pandas as pd\n",
    "\n",
    "@hpat.jit(locals={'vals:input': 'distributed', 'inds:input': 'distributed'})\n",
    "def get_stats(vals, inds):\n",
    "    df = pd.DataFrame({'vals': vals, 'classes': inds})\n",
    "    stat = df.describe()\n",
    "    print(stat)\n",
    "    TRUCK = 717\n",
    "    print((inds == TRUCK).sum())\n",
    "\n",
    "get_stats(vals.data.numpy(), inds.data.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to distributed return annotation, distributed inputs are annotated as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
